{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 57\n",
    "SAMPLING_FREQUENCY = 173.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set random seed into fixed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "x = pickle.load(open('../data/x.pkl', 'rb'))\n",
    "y = pickle.load(open('../data/y.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4097)\n",
      "(100, 4097)\n"
     ]
    }
   ],
   "source": [
    "x_normal = np.concatenate((x[:300], x[400:]), axis=0)\n",
    "x_seizure = x[300:400]\n",
    "\n",
    "print(x_normal.shape)\n",
    "print(x_seizure.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4097)\n",
      "(100, 4097)\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "b, a = butter(3, [0.5,40], btype='bandpass',fs=SAMPLING_FREQUENCY)\n",
    "\n",
    "x_normal_filtered = np.array([lfilter(b,a,x_normal[index,:]) for index in range(x_normal.shape[0])])\n",
    "x_seizure_filtered = np.array([lfilter(b,a,x_seizure[index,:]) for index in range(x_seizure.shape[0])])\n",
    "\n",
    "print(x_normal.shape)\n",
    "print(x_seizure.shape)\n",
    "\n",
    "x_normal = x_normal_filtered\n",
    "x_seizure = x_seizure_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract statistics features from dataset such as mean, std, var, min, max, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    return np.mean(data, axis=-1)\n",
    "\n",
    "def std(data):\n",
    "    return np.std(data, axis=-1)\n",
    "\n",
    "def ptp(data):\n",
    "    return np.ptp(data, axis=-1)\n",
    "\n",
    "def var(data):\n",
    "    return np.var(data, axis=-1)\n",
    "\n",
    "def minimum(data):\n",
    "    return np.min(data, axis=-1)\n",
    "\n",
    "def maximum(data):\n",
    "    return np.max(data, axis=-1)\n",
    "\n",
    "def arg_min(data):\n",
    "    return np.argmin(data, axis=-1)\n",
    "\n",
    "def arg_max(data):\n",
    "    return np.argmax(data, axis= -1)\n",
    "\n",
    "def sqrt(data):\n",
    "    return np.sqrt(np.mean(data**2, axis=-1))\n",
    "\n",
    "def abs_diff_signal(data):\n",
    "    return np.sum(np.abs(np.diff(data, axis=-1)), axis=-1)\n",
    "\n",
    "def concat_statistics_features(data):\n",
    "    return (mean(data), var(data), std(data), minimum(data), maximum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics_features = []\n",
    "for data in x_normal:\n",
    "    statistics_features.append(concat_statistics_features(data))\n",
    "\n",
    "statistics_features_array = np.array(statistics_features)\n",
    "statistics_features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: antropy in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: stochastic in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from antropy) (0.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from antropy) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from antropy) (1.22.4)\n",
      "Requirement already satisfied: numba in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from antropy) (0.56.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from antropy) (1.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba->antropy) (58.1.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba->antropy) (0.39.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->antropy) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sq-pc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn->antropy) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install antropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract entropy features from dataset such as permutation, singular value decomposition, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antropy as ent\n",
    "\n",
    "# Permutation entropy\n",
    "def permutation(data):\n",
    "    return ent.perm_entropy(data, normalize=True)\n",
    "\n",
    "# Spectral entropy\n",
    "def spectral(data):\n",
    "    return ent.spectral_entropy(data, sf=100, method='welch', normalize=True)\n",
    "\n",
    "# Singular value decomposition entropy\n",
    "def singular_value_decomposition(data):\n",
    "    return ent.svd_entropy(data, normalize=True)\n",
    "\n",
    "# Approximate entropy\n",
    "def approximate(data):\n",
    "    return ent.app_entropy(data)\n",
    "\n",
    "# Sample entropy\n",
    "def sample(data):\n",
    "    return ent.app_entropy(x)\n",
    "\n",
    "# Hjorth mobility and complexity\n",
    "def hjorth_mobility(data):\n",
    "    return ent.hjorth_params(x)\n",
    "\n",
    "# Number of zero-crossings\n",
    "def number_of_zerocross(data):\n",
    "    return ent.num_zerocross(x)\n",
    "\n",
    "# Lempel-Ziv complexity\n",
    "def lempel_ziv(data):\n",
    "    return ent.lziv_complexity('01111000011001', normalize=True)\n",
    "\n",
    "def concat_entropy_features(data):\n",
    "    return (permutation(data), spectral(data), singular_value_decomposition(data), approximate(data), lempel_ziv(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_features = []\n",
    "for data in x_normal:\n",
    "    entropy_features.append(concat_entropy_features(data))\n",
    "\n",
    "entropy_features_array = np.array(entropy_features)\n",
    "entropy_features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167.39828589112523\n",
      "3130\n",
      "0.05348188047639783\n"
     ]
    }
   ],
   "source": [
    "def ptp(data):\n",
    "    return np.ptp(data, axis=-1)\n",
    "\n",
    "def twoop(data):\n",
    "    return np.argmax(data) - np.argmin(data)\n",
    "\n",
    "def pps(data):\n",
    "    return ptp(data) / twoop(data)\n",
    "\n",
    "print(ptp(x_normal[0]))\n",
    "print(twoop(x_normal[0]))\n",
    "print(pps(x_normal[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f049f70bb1cc903f2a8aa743f371e4512dfa343431baa315c1f370fd7c1e81b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
